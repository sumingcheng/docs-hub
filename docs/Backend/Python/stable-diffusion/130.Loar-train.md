## SD 微调模型

### 文本反转（Textual Inversion, 也叫嵌入）

- 文本反转通过从少量示例图像中捕获新概念。它学习了一个新文本标记的令牌嵌入，同时保持模型的其余组件不变。此方法不会修改原始模型，但使其能够从少数图像中理解新概念，从而基于学习到的嵌入生成清晰度更高、噪声更少的图像

### Dreambooth

- Dreambooth 使用主题的少量图像个性化文本到图像模型，例如稳定扩散模型，从而允许在不同的场景、姿势和视图中生成主题的上下文图像。与文本反转不同，Dreambooth 微调整个文本到图像模型。它需要更多的内存，使其对于业余用户来说更加显存密集和成本高昂，尽管优化可以使其在 16 GB 的 GPU 上进行训练

### LoRA

- LoRA（低秩适应）通过使用少量高质量图像训练网络的某些层。与完整模型微调相比，它是一种轻量级方法，使训练更容易且资源需求较少。通过 LoRA 添加的新层在推断过程中替换基础模型的一些层，因此其有效性取决于基础模型

### Hypernetworks

- Hypernetworks 用于微调稳定扩散图像生成，通过注入额外的小型神经网络来修改模型的样式，而不改变原始模型参数。该方法以资源效率高和训练快而闻名，使其成为稳定扩散模型的热门微调技术

从训练时间和适用性的角度来讲 loar 是毕竟流行的

## 基础要求

:::success
确保你的 Windows 系统是最新的，至少 Windows 7，但 Windows 10 更佳。安装 Python 3.6 或更高版本，因为 PyTorch 需要它来运行。

硬件方面，你需要一个较新的 CPU 和至少 8GB 的 RAM。如果打算用 GPU 加速，选 NVIDIA 显卡，最起码 8GB 以上显存。

软件安装方面，你需要 PyTorch 和一些 Python 库，比如 NumPy 和 SciPy。如果用到 GPU，还要安装合适的 CUDA 版本。

### 显卡推荐

- **NVIDIA GeForce RTX 30/40 系列**（如 RTX 3080 或 RTX 3090）：这些卡提供了极好的性能和显存，非常适合机器学习。RTX 4090 尤其强大
- **NVIDIA Titan RTX**：这是一款专业级显卡，拥有 24 GB 显存，非常适合处理大型数据集和复杂模型。
- **NVIDIA Quadro 系列**：Quadro 显卡通常用于专业工作站。它们具有大量显存和优化的驱动程序，适用于高端应用。

:::

## 素材准备

1. 十五张左右的高质量图片即可，什么叫高质量？就是清晰，图片不是特别大也不是特别小
2. 风格统一，图片内的杂物不要过多，保证训练的内容是尽可能单一的

### 素材处理

1. 目前使用底模是 SD1.5 裁切成 512\*512 的图片进行训练，当然大一些也可以，但是也需要更多显存和时间
2. 对图片进行标注，人工进行校验和修正，删除无用特征词

## 训练

这里使用[GitHub - bmaltais/kohya_ss](https://github.com/bmaltais/kohya_ss)大佬的项目进行训练

具体训练步骤和参数可以直接看阿里云的文档[使用 PAI-EAS 一键部署 Kohya SD 模型微调应用\_机器学习平台 PAI-阿里云帮助中心](https://help.aliyun.com/zh/pai/use-cases/deploy-a-kohya-sd-model-service-and-train-a-lora-model)

## 使用

1. 训练好的 loar 导入到 SD 中进行使用
2. 你可以把生成后喜欢的图片跟你的 loar 模型进行关联，起一样的名字即可
